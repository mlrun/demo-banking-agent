{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd0dfc6-4ed5-40f1-ab93-9fcb72fdb3b6",
   "metadata": {},
   "source": [
    "# Guardrail Deployment\n",
    "\n",
    "The second part of the demo is to deploy guardrails to be used later in our application pipeline to filter user inputs. This notebook will also deploy an LLM as a Judge monitoring application to monitor our generative input guardrail for banking topic adherence.\n",
    "\n",
    "In this notebook, you will:\n",
    "- Set up and configure project secrets and environment variables.\n",
    "- Deploy multiple guardrail functions, including banking-topic and toxicity filters.\n",
    "- Log and register models for use in the guardrail functions.\n",
    "- Demonstrate how to invoke and test the deployed guardrails.\n",
    "- Monitor the effectiveness of the guardrails using an LLM-based evaluation application.\n",
    "\n",
    "These steps ensure that only appropriate, banking-related, and non-toxic user inputs are processed by downstream applications.\n",
    "\n",
    "![](images/02_guardrail_deployment_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d7752a7-d47e-4b60-8ed3-77600e1d38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from mlrun.features import Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a763105c-c7b4-4fa4-b719-a371fd897f51",
   "metadata": {},
   "source": [
    "### Setup Project\n",
    "\n",
    "Load the previously created project in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ea8ae0f-2bde-49c7-ade1-932fff5d66ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Source: v3io:///bigdata/banking_agent.zip\n",
      "Exporting project as zip archive to v3io:///bigdata/banking_agent.zip...\n",
      "> 2025-07-18 18:39:36,413 [info] Project loaded successfully: {\"project_name\":\"banking-agent-v2\"}\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\"banking-agent-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdff0d6",
   "metadata": {},
   "source": [
    "### Set Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d968d75-ddff-41a2-835f-973bf995cc57",
   "metadata": {},
   "source": [
    "Set the OpenAI credentials in the project and local environment - **be sure to update [.env.example](.env.example) as described in the [README](README.md)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6505199-683f-403d-b245-80318b5936e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.set_secrets(file_path=\"ai_gateway.env\")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"ai_gateway.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8481d6-7361-4bfa-b705-0a5765399f48",
   "metadata": {},
   "source": [
    "### LLM as a Judge Monitoring Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f46ba",
   "metadata": {},
   "source": [
    "The \"LLM as a Judge\" monitoring application leverages a large language model (LLM) to automatically evaluate and score the effectiveness of deployed guardrails. By providing a rubric and clear examples, the LLM acts as an impartial evaluator, determining whether user inputs are correctly classified according to defined criteria (e.g., banking-topic relevance). This approach enables scalable, consistent, and automated assessment of guardrail performance, ensuring that only appropriate and relevant inputs are processed by downstream applications.\n",
    "\n",
    "This implementation is pulled from another [MLRun demo - LLM monitoring and feedback loop: Banking](https://github.com/mlrun/demo-monitoring-and-feedback-loop/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec36324-8e80-4517-8840-11d1b1fba0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict_to_banking_config = {\n",
    "    \"name\": \"Restrict to banking\",\n",
    "    \"definition\": \"This metric evaluates whether the model correctly classifies a question as banking-related (`True`) or not (`False`). The model must respond with only a boolean. Do not answer the question or explain the classification—only the correctness of the True/False label matters.\",\n",
    "    \"rubric\": \"\"\"\n",
    "Scoring:\n",
    "- Score 0 (Incorrect): The model incorrectly classifies the question (e.g., labels a non-banking question as `True`, or a banking question as `False`).\n",
    "- Score 1 (Correct): The model correctly classifies the question (e.g., `True` for banking-related, `False` for unrelated).\n",
    "\"\"\",\n",
    "    \"examples\": \"\"\"\n",
    "Question: What is the process to apply for a mortgage?\n",
    "    Correct: True\n",
    "    Incorrect: False\n",
    "Question: How tall is the Empire State Building?\n",
    "    Correct: False\n",
    "    Incorrect: True\n",
    "Question: What is the process to apply for a checking account?\n",
    "    Correct: True\n",
    "    Incorrect: False\n",
    "Question: What is the best recipe for chocolate cake?\n",
    "    Correct: False\n",
    "    Incorrect: True\n",
    "\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1566010",
   "metadata": {},
   "source": [
    "More information about model monitoring applications in the context of LLM's can be found in the [documentation](https://docs.mlrun.org/en/stable/tutorials/genai-02-model-monitor-llm.html#genai-02-mm-llm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23093d1-00eb-4358-8010-2bbddbb701d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_app = project.set_model_monitoring_function(\n",
    "    func=\"src/functions/llm_as_a_judge.py\",\n",
    "    application_class=\"LLMAsAJudgeApplication\",\n",
    "    name=\"restrict-to-banking-guardrail\",\n",
    "    image=\"mlrun/mlrun:1.8.0\",\n",
    "    framework=\"openai\",\n",
    "    judge_type=\"single-grading\",\n",
    "    metric_name=\"restrict_to_banking\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    prompt_config=restrict_to_banking_config,\n",
    "    requirements=[\"openai\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38d1f84f-6ec5-4c61-9417-b99fd36738ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-11 21:29:30,950 [info] Starting remote function deploy\n",
      "2025-07-11 21:29:31  (info) Deploying function\n",
      "2025-07-11 21:29:31  (info) Building\n",
      "2025-07-11 21:29:32  (info) Staging files and preparing base images\n",
      "2025-07-11 21:29:32  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2025-07-11 21:29:32  (info) Building processor image\n",
      "2025-07-11 21:31:02  (info) Build complete\n",
      "2025-07-11 21:31:18  (info) Function deploy complete\n",
      "> 2025-07-11 21:31:22,897 [info] Model endpoint creation task completed with state succeeded\n",
      "> 2025-07-11 21:31:22,898 [info] Successfully deployed function: {\"external_invocation_urls\":[],\"internal_invocation_urls\":[\"nuclio-banking-agent-v2-restrict-to-banking-guardrail.default-tenant.svc.cluster.local:8080\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeployStatus(state=ready, outputs={'endpoint': 'http://nuclio-banking-agent-v2-restrict-to-banking-guardrail.default-tenant.svc.cluster.local:8080', 'name': 'banking-agent-v2-restrict-to-banking-guardrail'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.deploy_function(monitoring_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73812b75-e859-43c4-8ed1-3bd88b2d1f28",
   "metadata": {},
   "source": [
    "### Banking Topic Guardrail\n",
    "\n",
    "The Banking Topic Guardrail is an LLM-powered filter designed to ensure that only banking-related user inputs are processed by downstream applications. It acts as a first line of defense, automatically classifying each user message as either relevant (`True`) or irrelevant (`False`) to banking topics, based on the context of the entire conversation.\n",
    "\n",
    "It's important to distinguish between the guardrail itself (this component), which enforces topic adherence in real time within the application, and the monitoring application described above. The monitoring application uses an LLM as a \"judge\" to independently evaluate and score the effectiveness of this guardrail, providing oversight and ensuring that the guardrail is functioning as intended. This separation allows for both proactive filtering and ongoing quality assurance of user input handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280321d7-98f8-4479-a84b-5ce52bc8e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_GUARDRAILS_V2 = \"\"\"\n",
    "You are input guardrails for an AI banking agent that responds exclusively to questions pertaining to banking topics. Respond only with a boolean true/false value on whether the input adheres to banking topics. Consider the most recent input in the context of the whole conversation. Do not include any pre or post amble.\n",
    "\n",
    "Examples:\n",
    "Q: What is the process to apply for a mortgage?\n",
    "A: True\n",
    "\n",
    "Q: What is the best recipe for chocolate cake?\n",
    "A: False\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_GUARDRAILS_V3 = \"\"\"\n",
    "You are a “banking‐topic guardrail” whose job is to scan a multi‐turn conversation and answer (strictly) “True” or “False” depending on whether the **latest user message** is about banking. \n",
    "\n",
    " • Always judge in context of the **entire** conversation history, but your answer hinges on whether the most recent user utterance is a banking question/request.  \n",
    " • Banking topics include (but are not limited to): mortgages, checking/savings accounts, loans, credit cards, interest rates, deposits, withdrawals, online/mobile banking, etc.  \n",
    " • If the last user message drifts off into any non‐banking domain—recipes, movies, sports, medical advice, etc.—you must output “False.”  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a13d05b3-3bb4-4a37-bf6a-6f9cfbdeb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the model to the project:\n",
    "model_artifact = project.log_model(\n",
    "    \"banking-topic-guardrail\",\n",
    "    model_file=\"src/no-op.pkl\",\n",
    "    inputs=[Feature(value_type=\"str\", name=\"question\")],\n",
    "    outputs=[Feature(value_type=\"str\", name=\"answer\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "184e70ee-7bf4-45ee-b53d-f1d71f93c356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://models/banking-agent-v2/banking-topic-guardrail#0@55ca13e6b3067932a840960cc8f52921f7079740^334e112a3e3844db826e0ce2f4088fe9a1c7150b'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_artifact.uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66bcfdc-1a95-4589-9975-3be756f169ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_topic_guardrail = project.set_function(\n",
    "    name=\"banking-topic-guardrail-v2\",\n",
    "    func=\"src/functions/banking_topic_guardrail.py\",\n",
    "    kind=\"serving\",\n",
    "    image=\"mlrun/mlrun:1.8.0\",\n",
    "    requirements=[\"openai\"],\n",
    ")\n",
    "banking_topic_guardrail.add_model(\n",
    "    \"banking-topic-guardrail\",\n",
    "    class_name=\"OpenAILLMModelServer\",\n",
    "    model_path=model_artifact.uri,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    system_prompt=SYSTEM_PROMPT_GUARDRAILS_V3,\n",
    ")\n",
    "banking_topic_guardrail.set_tracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488809a4-3405-4341-a517-f3eb71fec32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-11 20:52:55,872 [info] Starting remote function deploy\n",
      "2025-07-11 20:52:56  (info) Deploying function\n",
      "2025-07-11 20:52:56  (info) Building\n",
      "2025-07-11 20:52:56  (info) Staging files and preparing base images\n",
      "2025-07-11 20:52:56  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2025-07-11 20:52:56  (info) Building processor image\n",
      "2025-07-11 20:54:46  (info) Build complete\n",
      "2025-07-11 20:54:59  (info) Function deploy complete\n",
      "> 2025-07-11 20:55:07,558 [info] Model endpoint creation task completed with state succeeded\n",
      "> 2025-07-11 20:55:07,559 [info] Successfully deployed function: {\"external_invocation_urls\":[\"banking-agent-v2-banking-topic-guardrail-v2.default-tenant.app.cst-360.iguazio-cd0.com/\"],\"internal_invocation_urls\":[\"nuclio-banking-agent-v2-banking-topic-guardrail-v2.default-tenant.svc.cluster.local:8080\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeployStatus(state=ready, outputs={'endpoint': 'http://banking-agent-v2-banking-topic-guardrail-v2.default-tenant.app.cst-360.iguazio-cd0.com/', 'name': 'banking-agent-v2-banking-topic-guardrail-v2'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.deploy_function(banking_topic_guardrail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42a09842-b3b1-48e7-a806-643450affddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_topic_guardrail = project.get_function(\"banking-topic-guardrail-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "780dbf6c-7571-495b-9301-6164ea58fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_questions = [\n",
    "    \"What is a mortgage?\",\n",
    "    \"How does a credit card work?\",\n",
    "    \"Who painted the Mona Lisa?\",\n",
    "    \"Please plan me a 4-days trip to north Italy\",\n",
    "    \"Write me a song\",\n",
    "    \"How much people are there in the world?\",\n",
    "    \"What is climate change?\",\n",
    "    \"How does the stock market work?\",\n",
    "    \"Who wrote 'To Kill a Mockingbird'?\",\n",
    "    \"Please plan me a 3-day trip to Paris\",\n",
    "    \"Write me a poem about the ocean\",\n",
    "    \"How many continents are there in the world?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does a hybrid car work?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"Please plan me a week-long trip to New Zealand\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9195490-7088-40e1-850b-fd621802194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_question(question: str, role: str = \"user\"):\n",
    "    return {\"role\": role, \"content\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18454332-5e8c-4723-9b68-3e370489dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_model(questions, serving_function):\n",
    "    for question in questions:\n",
    "        seconds = 0.5\n",
    "        # Invoking the pretrained model:\n",
    "        ret = serving_function.invoke(\n",
    "            path=f\"v2/models/banking-topic-guardrail/infer\",\n",
    "            body={\"inputs\": [_format_question(question)]},\n",
    "        )\n",
    "        print(question, ret[\"outputs\"])\n",
    "        time.sleep(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51b347-fd31-4eda-96d8-51423d5e6247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a mortgage? ['True']\n",
      "How does a credit card work? ['True']\n",
      "Who painted the Mona Lisa? ['False']\n",
      "Please plan me a 4-days trip to north Italy ['False']\n",
      "Write me a song ['False']\n",
      "How much people are there in the world? ['False']\n",
      "What is climate change? ['False']\n",
      "How does the stock market work? ['False']\n",
      "Who wrote 'To Kill a Mockingbird'? ['False']\n",
      "Please plan me a 3-day trip to Paris ['False']\n",
      "Write me a poem about the ocean ['False']\n",
      "How many continents are there in the world? ['False']\n",
      "What is artificial intelligence? ['False']\n",
      "How does a hybrid car work? ['False']\n",
      "Who invented the telephone? ['False']\n",
      "Please plan me a week-long trip to New Zealand ['False']\n",
      "What is a mortgage? ['True']\n",
      "How does a credit card work? ['True']\n",
      "Who painted the Mona Lisa? ['False']\n",
      "Please plan me a 4-days trip to north Italy ['False']\n",
      "Write me a song ['False']\n",
      "How much people are there in the world? ['False']\n",
      "What is climate change? ['False']\n",
      "How does the stock market work? ['False']\n",
      "Who wrote 'To Kill a Mockingbird'? ['False']\n",
      "Please plan me a 3-day trip to Paris ['False']\n",
      "Write me a poem about the ocean ['False']\n",
      "How many continents are there in the world? ['False']\n",
      "What is artificial intelligence? ['False']\n",
      "How does a hybrid car work? ['False']\n",
      "Who invented the telephone? ['False']\n",
      "Please plan me a week-long trip to New Zealand ['False']\n",
      "What is a mortgage? ['True']\n",
      "How does a credit card work? ['True']\n",
      "Who painted the Mona Lisa? ['False']\n",
      "Please plan me a 4-days trip to north Italy ['False']\n",
      "Write me a song ['False']\n",
      "How much people are there in the world? ['False']\n",
      "What is climate change? ['False']\n",
      "How does the stock market work? ['False']\n",
      "Who wrote 'To Kill a Mockingbird'? ['False']\n",
      "Please plan me a 3-day trip to Paris ['False']\n",
      "Write me a poem about the ocean ['False']\n",
      "How many continents are there in the world? ['False']\n",
      "What is artificial intelligence? ['False']\n",
      "How does a hybrid car work? ['False']\n",
      "Who invented the telephone? ['False']\n",
      "Please plan me a week-long trip to New Zealand ['False']\n",
      "What is a mortgage? ['True']\n",
      "How does a credit card work? ['True']\n",
      "Who painted the Mona Lisa? ['False']\n",
      "Please plan me a 4-days trip to north Italy ['False']\n",
      "Write me a song ['False']\n",
      "How much people are there in the world? ['False']\n",
      "What is climate change? ['False']\n",
      "How does the stock market work? ['False']\n",
      "Who wrote 'To Kill a Mockingbird'? ['False']\n",
      "Please plan me a 3-day trip to Paris ['False']\n",
      "Write me a poem about the ocean ['False']\n",
      "How many continents are there in the world? ['False']\n",
      "What is artificial intelligence? ['False']\n",
      "How does a hybrid car work? ['False']\n",
      "Who invented the telephone? ['False']\n",
      "Please plan me a week-long trip to New Zealand ['False']\n",
      "What is a mortgage? ['True']\n",
      "How does a credit card work? ['True']\n",
      "Who painted the Mona Lisa? ['False']\n",
      "Please plan me a 4-days trip to north Italy ['False']\n",
      "Write me a song ['False']\n",
      "How much people are there in the world? ['False']\n",
      "What is climate change? ['False']\n",
      "How does the stock market work? ['False']\n",
      "Who wrote 'To Kill a Mockingbird'? ['False']\n",
      "Please plan me a 3-day trip to Paris ['False']\n",
      "Write me a poem about the ocean ['False']\n",
      "How many continents are there in the world? ['False']\n",
      "What is artificial intelligence? ['False']\n",
      "How does a hybrid car work? ['False']\n",
      "Who invented the telephone? ['False']\n",
      "Please plan me a week-long trip to New Zealand ['False']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    question_model(\n",
    "        questions=example_questions,\n",
    "        serving_function=banking_topic_guardrail,\n",
    "    )\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acfe0f",
   "metadata": {},
   "source": [
    "Once the guardrail is deployed and invoked, you will be able to view the model monitoring results in the MLRun UI:\n",
    "![](images/generative_model_monitoring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac63e4d8-e9c7-487f-b4a8-6f410c0df1cb",
   "metadata": {},
   "source": [
    "### Toxicity Filter Guardrail\n",
    "\n",
    "The Toxicity Filter Guardrail is designed to automatically detect and filter out user inputs that contain toxic, offensive, or inappropriate language. By leveraging a toxicity classification model, this guardrail ensures that only safe and respectful messages are processed by downstream applications. This helps maintain a positive user experience and protects the system from harmful or disruptive content. The toxicity filter can be customized with a threshold to determine the sensitivity of the filter, allowing for flexible adaptation to different application requirements.\n",
    "\n",
    "The output of the toxicity guardrail is a boolean value (`True` or `False`). A result of `True` means the input passes the guardrail (i.e., is non-toxic and allowed through), while `False` indicates the input is flagged as toxic and is blocked from further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d748dc-a217-4206-a2ff-0d7bd3123b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.serving.states.TaskStep at 0x7f2151ce3fa0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_guardrail = project.set_function(\n",
    "    name=\"toxicity-guardrail\",\n",
    "    func=\"src/functions/toxicity_guardrail.py\",\n",
    "    kind=\"serving\",\n",
    "    requirements=[\"evaluate\"],\n",
    ")\n",
    "toxicity_guardrail.add_model(\n",
    "    \"toxicity-guardrail\",\n",
    "    class_name=\"ToxicityClassifierModelServer\",\n",
    "    threshold=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b5f4bed-7b96-4839-8dc0-c4beff398267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2025-07-11 20:57:28,206 [info] Starting remote function deploy\n",
      "2025-07-11 20:57:28  (info) Deploying function\n",
      "2025-07-11 20:57:28  (info) Building\n",
      "2025-07-11 20:57:28  (info) Staging files and preparing base images\n",
      "2025-07-11 20:57:28  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2025-07-11 20:57:28  (info) Building processor image\n",
      "2025-07-11 21:01:28  (info) Build complete\n",
      "2025-07-11 21:01:58  (info) Function deploy complete\n",
      "> 2025-07-11 21:02:00,313 [info] Model endpoint creation task completed with state succeeded\n",
      "> 2025-07-11 21:02:00,314 [info] Successfully deployed function: {\"external_invocation_urls\":[\"banking-agent-v2-toxicity-guardrail.default-tenant.app.cst-360.iguazio-cd0.com/\"],\"internal_invocation_urls\":[\"nuclio-banking-agent-v2-toxicity-guardrail.default-tenant.svc.cluster.local:8080\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeployStatus(state=ready, outputs={'endpoint': 'http://banking-agent-v2-toxicity-guardrail.default-tenant.app.cst-360.iguazio-cd0.com/', 'name': 'banking-agent-v2-toxicity-guardrail'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.deploy_function(toxicity_guardrail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17542798-45d9-4705-89c1-d0ba936d9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_guardrail = project.get_function(\"toxicity-guardrail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eeb776-4b50-4c6e-9853-f86cb597b052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cb1ceae3-59a5-4ddc-817a-391a6243db22',\n",
       " 'model_name': 'toxicity-guardrail',\n",
       " 'outputs': [True],\n",
       " 'timestamp': '2025-07-11 21:03:03.676837+00:00',\n",
       " 'model_endpoint_uid': '7e38ffa73f894252b20eaaed93bca919'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_guardrail.invoke(path=\"/\", body={\"inputs\": [_format_question(\"hello there!\")]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banking-agent",
   "language": "python",
   "name": "conda-env-.conda-banking-agent-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
